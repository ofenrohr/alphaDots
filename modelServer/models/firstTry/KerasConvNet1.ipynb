{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Fully Convolutional Network\n",
    "\n",
    "First try to train a fcn to play Dots and Boxes.\n",
    "\n",
    "## Data\n",
    "\n",
    "We have grayscale images as input and output. The input images shows all dots, lines and boxes while the output image only shows the line the hard AI in KSquares would choose. For this first try there are 10,000 examples of 5x4 games that are in the \"interesting\" phase of the game. For a more detailed description of how this data was generated, have a look at the \"convertDataset\" directory.\n",
    "\n",
    "Input: ![input image data](input.png) Output: ![output image data](output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "import keras.backend as K\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n"
     ]
    }
   ],
   "source": [
    "print(K.image_data_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Keras Model\n",
    "\n",
    "For this first try we use 6 convolutional layers with 64 filters, combined with relu activation. We do batch normalization after each layer.\n",
    "We want to use categorical crossentropy as the loss function, which expects our output to be in one-hot encoding. There are two classes: \"line\" and \"no line\". Thus our last layer reduces the output to two channels. The last layer uses softmax activation for two reasons: \n",
    "\n",
    "1. the loss function expects the softmax output\n",
    "2. a probability distribution over the possible actions fits our needs quite well\n",
    "\n",
    "Since the model is a fully convolutional network it is not bound to a specific input size. That's pretty useful because Dots and Boxes is not bound to be played on a particularly sized board. It's a key improvement over the network used in QDab.\n",
    "\n",
    "![firstTry model architecture](firstTry_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, None, None, 64)    640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, None, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, None, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, None, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, None, None, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, None, None, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, None, None, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, None, None, 2)     1154      \n",
      "=================================================================\n",
      "Total params: 187,970\n",
      "Trainable params: 187,202\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#img_input = Input(shape=(11,13,1,))\n",
    "img_input = Input(shape=(None,None,1,))\n",
    "x = Conv2D(64, (3,3), padding='same', activation='relu')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(2, (3,3), padding='same', activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=img_input, outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "The dataset has been created with the convertDataset.py file. It contains two numpy arrays with grayscale images, using 8 bits per pixel. \n",
    "\n",
    "Here, the dataset is converted to match the network's requirements by converting it to floating point data and by normalizing the data to be in the range from 0 to 1. Furthermore the target array is converted to the one-hot format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data:\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 215 255 215 255 215 255 215 255 215 255 215   0]\n",
      " [  0 255   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 215   0 215 255 215 255 215 255 215 255 215   0]\n",
      " [  0   0   0 255   0   0   0 255   0   0   0 255   0]\n",
      " [  0 215 255 215   0 215   0 215   0 215   0 215   0]\n",
      " [  0 255   0   0   0 255   0 255   0   0   0 255   0]\n",
      " [  0 215   0 215 255 215   0 215 255 215 255 215   0]\n",
      " [  0 255   0   0   0   0   0 255  65 255   0   0   0]\n",
      " [  0 215 255 215 255 215 255 215 255 215 255 215   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 255   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "(10069, 11, 13)\n",
      "(10069, 11, 13)\n",
      "\n",
      "normalized data:\n",
      "[[[ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      "  [ 0.    0.84  1.    0.84  0.    0.84  1.    0.84  1.    0.84  0.  ]\n",
      "  [ 0.    1.    0.    0.    0.    1.    0.    0.    0.    1.    0.  ]\n",
      "  [ 0.    0.84  0.    0.84  1.    0.84  0.    0.84  0.    0.84  0.  ]\n",
      "  [ 0.    1.    0.    1.    0.    0.    0.    1.    0.    1.    0.  ]\n",
      "  [ 0.    0.84  0.    0.84  0.    0.84  1.    0.84  0.    0.84  0.  ]\n",
      "  [ 0.    1.    0.    1.    0.    0.    0.    0.    0.    1.    0.  ]\n",
      "  [ 0.    0.84  0.    0.84  1.    0.84  1.    0.84  1.    0.84  0.  ]\n",
      "  [ 0.    1.    0.    1.    0.    0.    0.    1.    0.25  1.    0.  ]\n",
      "  [ 0.    0.84  0.    0.84  0.    0.84  0.    0.84  1.    0.84  0.  ]\n",
      "  [ 0.    1.    0.    1.    0.    0.    0.    1.    0.    1.    0.  ]\n",
      "  [ 0.    0.84  0.    0.84  1.    0.84  1.    0.84  0.    0.84  0.  ]\n",
      "  [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]]\n",
      "[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]]\n",
      "(10069, 11, 13, 1)\n",
      "(10069, 11, 13, 1)\n"
     ]
    }
   ],
   "source": [
    "firstTryDataset = np.load('firstTry.npz')\n",
    "x_train = firstTryDataset['x_train']\n",
    "y_train = firstTryDataset['y_train']\n",
    "\n",
    "print(\"original data:\")\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "print(\"\\nnormalized data:\")\n",
    "sp = x_train.shape\n",
    "x_train = x_train.reshape((sp[0],sp[1],sp[2],1))\n",
    "sp = y_train.shape\n",
    "y_train = y_train.reshape((sp[0],sp[1],sp[2],1))\n",
    "\n",
    "x_train = x_train.astype(K.floatx())\n",
    "y_train = y_train.astype(K.floatx())\n",
    "#x_train = np.true_divide(x_train,255.0)\n",
    "x_train /= 255\n",
    "y_train /= 255\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "print(np.transpose(x_train[0]))\n",
    "print(np.transpose(y_train[0]))\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10069, 11, 13, 2)\n",
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "y_cat = to_categorical(y_train).reshape(y_train.shape[0], y_train.shape[1], y_train.shape[2], 2)\n",
    "print(y_cat.shape)\n",
    "print(y_cat[0,:,:,0])\n",
    "print(y_cat[0,:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Train the model in 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10069/10069 [==============================] - 10s - loss: 0.1337    \n",
      "Epoch 2/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0300     \n",
      "Epoch 3/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0236     \n",
      "Epoch 4/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0207     \n",
      "Epoch 5/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0190     \n",
      "Epoch 6/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0179     \n",
      "Epoch 7/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0171     \n",
      "Epoch 8/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0164     \n",
      "Epoch 9/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0158     \n",
      "Epoch 10/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0154     \n",
      "Epoch 11/50\n",
      "10069/10069 [==============================] - 10s - loss: 0.0150    \n",
      "Epoch 12/50\n",
      "10069/10069 [==============================] - 10s - loss: 0.0146    \n",
      "Epoch 13/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0143     \n",
      "Epoch 14/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0140     \n",
      "Epoch 15/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0137     \n",
      "Epoch 16/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0135     \n",
      "Epoch 17/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0132     \n",
      "Epoch 18/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0130     \n",
      "Epoch 19/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0128     \n",
      "Epoch 20/50\n",
      "10069/10069 [==============================] - 10s - loss: 0.0126    \n",
      "Epoch 21/50\n",
      "10069/10069 [==============================] - 10s - loss: 0.0124    \n",
      "Epoch 22/50\n",
      "10069/10069 [==============================] - 10s - loss: 0.0123    \n",
      "Epoch 23/50\n",
      "10069/10069 [==============================] - 10s - loss: 0.0121    \n",
      "Epoch 24/50\n",
      "10069/10069 [==============================] - 10s - loss: 0.0120    \n",
      "Epoch 25/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0118     \n",
      "Epoch 26/50\n",
      "10069/10069 [==============================] - 10s - loss: 0.0117    \n",
      "Epoch 27/50\n",
      "10069/10069 [==============================] - 10s - loss: 0.0115    \n",
      "Epoch 28/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0114     \n",
      "Epoch 29/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0113     \n",
      "Epoch 30/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0112     \n",
      "Epoch 31/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0110     \n",
      "Epoch 32/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0109     \n",
      "Epoch 33/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0108     \n",
      "Epoch 34/50\n",
      "10069/10069 [==============================] - 10s - loss: 0.0107    \n",
      "Epoch 35/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0106     \n",
      "Epoch 36/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0105     \n",
      "Epoch 37/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0104     \n",
      "Epoch 38/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0103     \n",
      "Epoch 39/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0102     \n",
      "Epoch 40/50\n",
      "10069/10069 [==============================] - 8s - loss: 0.0102     \n",
      "Epoch 41/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0101     \n",
      "Epoch 42/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0100     \n",
      "Epoch 43/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0099     \n",
      "Epoch 44/50\n",
      "10069/10069 [==============================] - 8s - loss: 0.0098     \n",
      "Epoch 45/50\n",
      "10069/10069 [==============================] - 8s - loss: 0.0097     \n",
      "Epoch 46/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0097     \n",
      "Epoch 47/50\n",
      "10069/10069 [==============================] - 10s - loss: 0.0096    \n",
      "Epoch 48/50\n",
      "10069/10069 [==============================] - 9s - loss: 0.0095     \n",
      "Epoch 49/50\n",
      "10069/10069 [==============================] - 8s - loss: 0.0094     \n",
      "Epoch 50/50\n",
      "10069/10069 [==============================] - 8s - loss: 0.0094     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21013fdb90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_cat, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the model\n",
    "\n",
    "Here we make a prediction i.e. ask the model which line it would choose. \n",
    "\n",
    "To better visualize the model's decision, the input data and prediction are merged into one image. Since both input data and prediction are grayscale, they are merged into a RGB images. The input data is put into the red-channel while the prediction goes into the green-channel. \n",
    "\n",
    "The 10th example is an instance where double-dealing is the right choice to win the game. Instead of simply taking the four boxes, the wise move is to decline them by double dealing, leaving two groups with two boxes each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0  84 100  84 100  84 100  84 100  84 100  84   0]\n",
      " [  0   0   0   0   0   0   0 100   0   0   0 100   0]\n",
      " [  0  84 100  84 100  84   0  84   0  84 100  84   0]\n",
      " [  0 100   0   0   0 100   0 100   0   0   0 100   0]\n",
      " [  0  84   0  84   0  84   0  84 100  84 100  84   0]\n",
      " [  0 100   0 100   0   0   0 100   0   0   0 100   0]\n",
      " [  0  84   0  84 100  84 100  84   0  84   0  84   0]\n",
      " [  0 100   0   0   0   0   0   0   0 100   0 100   0]\n",
      " [  0  84 100  84 100  84 100  84 100  84   0  84   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "prediction: \n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  8  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 94  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  3  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIIAAABuCAIAAABk51xTAAABrElEQVR4nO3cMUoEQRBA0VUMDAXx\nDnr/w6x3EMHQSE2NqoVm9K+8lxZNK58JhmLndAIAAAB+5moen8fp4zh9dvabp3F6PU75JTIkyJAg\nQ4IMCTIkyJAgQ4IMCYu36MX8c5zOiT+Ounf+oxb/8MM4fTnqXk9DggwJMiTIkCBDggwJMiTIkCBD\nws08Po+vhovd7PievDg73jvvdeed8OltGp7vpunOLnrmaUiQIUGGBBkSZEiQIUGGBBkSZEhYrGa3\n9robdu7d+pvvx+nrUfd6GhJkSJAhQYYEGRJkSJAhQYYEGRIWu+jFfnWMeB530Vv75I2z87vu8/ie\nbBf9z8mQIEOCDAkyJMiQIEOCDAkyJBy4i77EPfZf3etpSJAhQYYEGRJkSJAhQYYEGRJkSNjbRW+c\nXeyEx+lxO+H5m+RP46vw/FvumachQYYEGRJkSJAhQYYEGRJkSJAh4cjfRe+sqnf80fe97aIvngwJ\nMiTIkCBDggwJMiTIkCBDwtYuerFP3vm+9zjd+b731h77djz7Ph+eeBoSZEiQIUGGBBkSZEiQIUGG\nBBkAAACAi/IF/Yg9k4Iv6awAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=130x110 at 0x7F21013A5950>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = 10 #random.randrange(x_train.shape[0])\n",
    "\n",
    "input_data = np.array([x_train[example]])\n",
    "\n",
    "\n",
    "prediction = model.predict(input_data)\n",
    "\n",
    "# print input data\n",
    "input_data_print = x_train[example,:,:,0] * 100\n",
    "input_data_print = input_data_print.astype(np.uint8)\n",
    "print(\"input: \")\n",
    "print(input_data_print)\n",
    "\n",
    "# generate greyscale image data from input data\n",
    "target_imgdata = x_train[example,:,:,0] * 255\n",
    "target_imgdata = target_imgdata.astype(np.uint8)\n",
    "\n",
    "# print prediction\n",
    "prediction_data_print = prediction[0,:,:,1] * 100 \n",
    "prediction_data_print = prediction_data_print.astype(np.uint8)\n",
    "print(\"prediction: \")\n",
    "print(prediction_data_print)\n",
    "\n",
    "# generate greyscale image data from prediction data\n",
    "prediction_imgdata = prediction[0] * 255\n",
    "prediction_imgdata = prediction_imgdata.astype(np.uint8)\n",
    "\n",
    "# merge image data in color channels\n",
    "tmp = np.zeros((prediction[0].shape[0], prediction[0].shape[1]), dtype=np.uint8)\n",
    "merged_imgdata = np.stack([target_imgdata, prediction_imgdata[:,:,1], tmp], axis=2)\n",
    "\n",
    "#create image\n",
    "img = Image.fromarray(merged_imgdata, 'RGB')\n",
    "img = img.resize(size=(img.size[0]*10, img.size[1]*10))\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Export the model\n",
    "Save the model. Export it with Keras's save method for use in the modelServer. The modelServer will provide the model's prediction function as a local Service for KSquares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = \"model/model-firstTry.h5\"\n",
    "model_json = model.save(model_path)\n",
    "img_path = \"model/model-firstTry.png\"\n",
    "plot_model(model, to_file=img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: model/tensorflow/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model/tensorflow/saved_model.pb'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature = tf.saved_model.signature_def_utils.predict_signature_def(\n",
    "    inputs={'image': model.input}, \n",
    "    outputs={'scores': model.output})     \n",
    "\n",
    "builder = tf.saved_model.builder.SavedModelBuilder('model/tensorflow')                                                                    \n",
    "builder.add_meta_graph_and_variables(                                                                                                        \n",
    "    sess=K.get_session(),                                                                                                                    \n",
    "    tags=[tf.saved_model.tag_constants.SERVING],                                                                                             \n",
    "    signature_def_map={                                                                                                                      \n",
    "        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:                                                                \n",
    "            signature                                                                                                                        \n",
    "    })                                                                                                                                       \n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
